{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Design and implementation of reinforcement learning games based on Q-**Learning**"
      ],
      "metadata": {
        "id": "4y0tIiyr_tG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2jCN1FW_qkD"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "***2.   Logging configuration and game constant definitions***\n",
        "\n",
        "\n",
        "Configure logging to display important information during training, which is convenient for debugging and tracking.\n",
        "It also defines the width, height, and grid size of the game window.\n",
        "It calculates the number of grids (horizontally and vertically).\n",
        "It defines common RGB color values ​​for drawing game elements and interfaces."
      ],
      "metadata": {
        "id": "TgOHVA54C2iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring Logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "# Game Constants\n",
        "WIDTH, HEIGHT = 1000, 800\n",
        "GRID_SIZE = 40\n",
        "GRID_WIDTH = (WIDTH - 200) // GRID_SIZE\n",
        "GRID_HEIGHT = HEIGHT // GRID_SIZE\n",
        "COLORS = {\n",
        "    'WHITE': (255, 255, 255),\n",
        "    'BLACK': (0, 0, 0),\n",
        "    'YELLOW': (255, 255, 0),\n",
        "    'RED': (255, 0, 0),\n",
        "    'GREEN': (0, 255, 0),\n",
        "    'BROWN': (139, 69, 19),\n",
        "    'GRAY': (128, 128, 128),\n",
        "    'DARK_GRAY': (50, 50, 50)\n",
        "}\n"
      ],
      "metadata": {
        "id": "NTjNkav4ApmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   ***Pygame initialization***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Initialize Pygame, set up the game window, title and clock.\n",
        "Load fonts for displaying game information and statistics."
      ],
      "metadata": {
        "id": "H3eT4-SQDYVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Pygame\n",
        "pygame.init()\n",
        "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
        "pygame.display.set_caption(\"Advanced Pac-Man Q-Learning\")\n",
        "clock = pygame.time.Clock()\n",
        "font = pygame.font.Font(None, 24)\n",
        "bold_font = pygame.font.Font(None, 32)\n",
        "\n",
        "# Loading image material\n",
        "try:\n",
        "    player_img = pygame.image.load(\"player.webp\")\n",
        "    enemy_img = pygame.image.load(\"enemy.webp\")\n",
        "    gold_img = pygame.image.load(\"gold.webp\")\n",
        "    logging.info(\"Image loaded successfully!\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Image loading failed: {e}\")\n",
        "    player_img = pygame.Surface((GRID_SIZE, GRID_SIZE))\n",
        "    player_img.fill(COLORS['YELLOW'])\n",
        "    enemy_img = pygame.Surface((GRID_SIZE, GRID_SIZE))\n",
        "    enemy_img.fill(COLORS['RED'])\n",
        "    gold_img = pygame.Surface((GRID_SIZE, GRID_SIZE))\n",
        "    gold_img.fill(COLORS['GREEN'])\n",
        "\n",
        "# Resize an image\n",
        "player_img = pygame.transform.scale(player_img, (GRID_SIZE, GRID_SIZE))\n",
        "enemy_img = pygame.transform.scale(enemy_img, (GRID_SIZE, GRID_SIZE))\n",
        "gold_img = pygame.transform.scale(gold_img, (GRID_SIZE, GRID_SIZE))"
      ],
      "metadata": {
        "id": "cXzqKSmOAzQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4.  Core parameters of reinforcement learning***\n",
        "\n",
        "\n",
        "Defines the core parameters of Q-Learning:\n",
        "\n",
        "\n",
        "\n",
        "*   alpha: learning rate, controls the algorithm's acceptance of new information.\n",
        "*   gamma: discount factor, determines the impact of future rewards on current decisions.\n",
        "*   epsilon: exploration rate, used for switching between greedy strategy and exploration strategy.\n",
        "*   epsilon_decay: decay coefficient of exploration rate, gradually reducing the probability of exploration.\n",
        "*   batch_size: sample size for batch training.\n",
        "*   target_update: update frequency of target network."
      ],
      "metadata": {
        "id": "OGQwUYb7DmhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinforcement Learning Core Parameters\n",
        "alpha = 0.7  # Initial learning rate\n",
        "gamma = 0.97  # discount factor\n",
        "epsilon = 1.0  # initial exploration rate\n",
        "epsilon_decay = 0.9995\n",
        "epsilon_min = 0.02  # basic minimum value\n",
        "adaptive_epsilon_min = 0.02  # dynamic minimum value\n",
        "min_alpha = 0.1\n",
        "batch_size = 256\n",
        "target_update = 75\n",
        "actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "success_threshold = 0.25\n",
        "boost_factor = 1.5\n",
        "FPS = 30"
      ],
      "metadata": {
        "id": "5fJzfvvIA12R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***5.Prioritized Experience Replay Buffer***\n",
        "\n",
        "\n",
        "Implemented a prioritized experience replay buffer to store and sample experiences (state, action, reward, next state, completion flag). Sample a batch of experiences from the buffer by priority and calculate importance sampling weights.\n",
        "*   Calculate sampling probability:\n",
        "\n",
        "Priorities are raised to the power of alpha (probs = priorities^alpha) to amplify the influence of high priority experiences.\n",
        "\n",
        "Normalize probabilities (probs /= probs.sum()) to ensure that the sum of probabilities is 1.\n",
        "\n",
        "Sampling by probability: Use np.random.choice to select the index of experience according to the probability distribution.\n",
        "\n",
        "*   Calculate weights:\n",
        "\n",
        "*   Formula: weights = (N * probs)^(-beta), where N is the current size of the buffer.\n",
        "\n",
        "Normalize weights (weights /= weights.max()) to range between [0, 1] to prevent high priority samples from dominating training.\n",
        "\n",
        "*   Return results: sampled experience, index and weight (for subsequent Q value update)."
      ],
      "metadata": {
        "id": "91Rc7TSTEZxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prioritize experience replay buffer\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.priorities = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        max_prio = max(self.priorities) if self.priorities else 1.0\n",
        "        self.buffer.append(experience)\n",
        "        self.priorities.append(max_prio)\n",
        "\n",
        "    def sample(self, batch_size, alpha=0.7, beta=0.6):\n",
        "        probs = np.array(self.priorities) ** alpha\n",
        "        probs /= probs.sum()\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "        return samples, indices, weights\n",
        "\n",
        "replay_buffer = PrioritizedReplayBuffer(10000)"
      ],
      "metadata": {
        "id": "za4AxtQZA5Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***6.Dual Q Network Architecture***\n",
        "Implemented a dual Q network architecture to reduce the overestimation problem of target Q values ​​in Q-Learning.\n",
        "main is the main Q table, used to select actions.\n",
        "target is the target Q table, used to calculate the target Q value.\n",
        "Steps:\n",
        "*   Create a priority experience replay buffer instance with a maximum capacity of 10,000.\n",
        "*  Initialize the two Q tables (main and target) of the dual Q network\n",
        "*   Copy the parameters of the main Q table (main) to the target Q table (target).\n",
        "\n",
        "*  Effect:\n",
        "Reduce the overestimation problem of Q values. Traditional Q learning uses a single network to select and evaluate actions at the same time, which is prone to overestimating action values. The dual Q network significantly reduces the estimation bias by separating the selection and evaluation processes."
      ],
      "metadata": {
        "id": "ER0wyeKvB8aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dual Q network architecture\n",
        "class DoubleQTable:\n",
        "    def __init__(self, state_dims, action_size):\n",
        "        self.main = np.random.uniform(-0.1, 0.1, state_dims + (action_size,))\n",
        "        self.target = np.copy(self.main)\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target = np.copy(self.main)"
      ],
      "metadata": {
        "id": "QOp93-YDA7jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***7.Information panel class***\n",
        "*  Displays key information during training, such as the current episode, score, epsilon, number of steps, average change in Q value, and success rate.\n",
        "*  Improve observability and visualization of the training process."
      ],
      "metadata": {
        "id": "Y2gRiE2qFbYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Information panel class\n",
        "class InfoPanel:\n",
        "    def __init__(self):\n",
        "        self.width = 200\n",
        "        self.height = HEIGHT\n",
        "        self.x = WIDTH - self.width\n",
        "        self.y = 0\n",
        "\n",
        "    def draw(self, screen, episode, score, epsilon, steps, avg_q, success_count):\n",
        "        pygame.draw.rect(screen, COLORS['DARK_GRAY'], (self.x, self.y, self.width, self.height))\n",
        "        title = bold_font.render(\"Game Info\", True, COLORS['WHITE'])\n",
        "        screen.blit(title, (self.x + 10, 20))\n",
        "\n",
        "        y_offset = 60\n",
        "        info_items = [\n",
        "            (\"Episode\", episode),\n",
        "            (\"Score\", f\"{score:.1f}\"),\n",
        "            (\"Epsilon\", f\"{epsilon:.4f}\"),\n",
        "            (\"Steps\", steps),\n",
        "            (\"Avg QΔ\", f\"{avg_q:.2f}\"),\n",
        "            (\"Success\", sum(success_count))\n",
        "        ]\n",
        "        for label, value in info_items:\n",
        "            text = font.render(f\"{label}: {value}\", True, COLORS['WHITE'])\n",
        "            screen.blit(text, (self.x + 10, y_offset))\n",
        "            y_offset += 35\n",
        "\n",
        "        logs_title = bold_font.render(\"Recent Logs\", True, COLORS['WHITE'])\n",
        "        screen.blit(logs_title, (self.x + 10, HEIGHT - 160))\n",
        "\n",
        "        log_y = HEIGHT - 120\n",
        "        for msg in log_messages:\n",
        "            log_text = font.render(msg, True, COLORS['WHITE'])\n",
        "            screen.blit(log_text, (self.x + 10, log_y))\n",
        "            log_y += 20"
      ],
      "metadata": {
        "id": "UhH_FNq_A7_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fc6DY1eTFiXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maze Generation\n",
        "def generate_maze(width, height):\n",
        "    maze = []\n",
        "    for y in range(height):\n",
        "        row = []\n",
        "        for x in range(width):\n",
        "            if x == 0 or x == width - 1 or y == 0 or y == height - 1:\n",
        "                row.append('#')\n",
        "            else:\n",
        "                row.append('.' if random.random() > 0.1 else '#')\n",
        "        maze.append(''.join(row))\n",
        "    return maze\n",
        "\n",
        "maze = generate_maze(GRID_WIDTH, GRID_HEIGHT)\n",
        "obstacles = [(x, y) for y, row in enumerate(maze) for x, char in enumerate(row) if char == '#']\n"
      ],
      "metadata": {
        "id": "bgMJE7tjA-tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***9.Pac-Man class***\n",
        "*  Initialization method: Initialize Pac-Man's initial state and history.\n",
        "\n",
        "self.reset(): Call the reset method to set the initial position and visit record.\n",
        "\n",
        "self.path_history: Record the movement path of the last 5 steps (double-ended queue, automatically eliminate old data).\n",
        "\n",
        "self.visited_grids: A two-dimensional array that records the number of visits to each grid (used for exploration reward calculation).\n",
        "\n",
        "*  Reset method function: Reset Pac-Man to the center of the maze and clear the visit record.\n",
        "\n",
        "Initial position: (GRID_WIDTH//2, GRID_HEIGHT//2), which is the center of the maze.\n",
        "\n",
        "*  Move method function: Update Pac-Man's position according to the action, and record the path and number of visits.\n",
        "\n",
        "   Logic:\n",
        "\n",
        "Calculate the new coordinates (new_x, new_y), and check whether it conflicts with obstacles (is_obstacle).\n",
        "\n",
        "Update the position self.x, self.y.\n",
        "\n",
        "Record the new position to the visited collection and path_history queue.\n",
        "\n",
        "Update the number of visits to the corresponding grid in visited_grids.\n",
        "\n",
        "*  State acquisition method\n",
        "Returns the 8-dimensional discrete representation of the current state for Q-Learning input. Through discretization and feature compression, the original state space (such as 20x15 grid → 300 possibilities) is greatly reduced to a trainable scale."
      ],
      "metadata": {
        "id": "GUEyeVtpGCEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_obstacle(x, y):\n",
        "    return (x, y) in obstacles\n",
        "\n",
        "# Pac-Man class\n",
        "class Pacman:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.path_history = deque(maxlen=5)\n",
        "        self.visited_grids = np.zeros((GRID_WIDTH, GRID_HEIGHT))  # Added access statistics matrix\n",
        "\n",
        "    def reset(self):\n",
        "        self.x, self.y = GRID_WIDTH // 2, GRID_HEIGHT // 2\n",
        "        self.visited = set()\n",
        "        self.visited.add((self.x, self.y))\n",
        "\n",
        "    def move(self, action):\n",
        "        new_x, new_y = self.x, self.y\n",
        "        if action == 'UP' and not is_obstacle(self.x, self.y - 1):\n",
        "            new_y -= 1\n",
        "        elif action == 'DOWN' and not is_obstacle(self.x, self.y + 1):\n",
        "            new_y += 1\n",
        "        elif action == 'LEFT' and not is_obstacle(self.x - 1, self.y):\n",
        "            new_x -= 1\n",
        "        elif action == 'RIGHT' and not is_obstacle(self.x + 1, self.y):\n",
        "            new_x += 1\n",
        "\n",
        "        self.x, self.y = new_x, new_y\n",
        "        self.visited.add((new_x, new_y))\n",
        "        self.path_history.append((new_x, new_y))\n",
        "        self.visited_grids[self.x][self.y] += 1  # Update Visit Count\n",
        "\n",
        "    def get_state(self):\n",
        "        px, py = self.x, self.y\n",
        "        return (\n",
        "            px % 3,\n",
        "            py % 3,\n",
        "            int(any(e.x == px for e in enemies)),\n",
        "            int(any(e.y == py for e in enemies)),\n",
        "            min(4, len(food.positions) // 2),\n",
        "            int(px > GRID_WIDTH // 2),\n",
        "            sum(1 for e in enemies if abs(e.x - px) < 3),\n",
        "            min(4, sum(1 for p in food.positions if abs(p[0] - px) < 4))  # 修正括号\n",
        "        )\n"
      ],
      "metadata": {
        "id": "SdaUsKiJBBjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***10.Food class: Food generation and collection***\n",
        "\n",
        "\n",
        "Function: Manage the location and collection logic of food in the game.\n",
        "\n",
        "Method details:\n",
        "*  __init__ Initialization:\n",
        "\n",
        "  Call the reset method to initialize the food location list self.positions.\n",
        "\n",
        "*  reset Reset:\n",
        "\n",
        "  Steps:\n",
        "    Traverse all the grids of the maze and select the passable area (i.e. the position where char == '.') as the candidate food location.\n",
        "\n",
        "    Randomly shuffle the candidate positions (random.shuffle) to ensure that the food distribution is different in each game.\n",
        "\n",
        "    Select the first 10 positions as the food for this game (self.positions[:10]).\n",
        "\n",
        "    Design significance:\n",
        "\n",
        "    Every time the game is reset, the food position is randomly generated to increase the diversity of the game.\n",
        "\n",
        "    Limit the number of food to 10 and clarify the game goal (collect all food).\n",
        "\n",
        "*  collect Collect food:\n",
        "\n",
        "  Input: Pac-Man's current position (x, y).\n",
        "\n",
        "  Logic:If there is food at this position, remove the position from self.positions and return True.Otherwise return False.\n",
        "\n",
        "  Function: Trigger reward calculation (such as bonus points after collecting food)."
      ],
      "metadata": {
        "id": "VlC5hAnZGmxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Food class\n",
        "class Food:\n",
        "    def __init__(self):\n",
        "        self.positions = []\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.positions = [(x, y) for y, row in enumerate(maze) for x, char in enumerate(row) if char == '.']\n",
        "        random.shuffle(self.positions)\n",
        "        self.positions = self.positions[:10]\n",
        "\n",
        "    def collect(self, x, y):\n",
        "        if (x, y) in self.positions:\n",
        "            self.positions.remove((x, y))\n",
        "            return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "Exr23w-8BFHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***11.Enemy class: Enemy movement and behavior***\n",
        "\n",
        "  Function: Manage enemy generation and movement strategy (combining player tracking and random movement).\n",
        "\n",
        "Method details:\n",
        "*  __init__ Initialization:\n",
        "\n",
        "  Call the reset method to generate the enemy's initial position.\n",
        "\n",
        "*  Reset Reset:\n",
        "\n",
        "  Logic:\n",
        "\n",
        "  Randomly generate the enemy's coordinates (x, y) through the while True loop until a non-obstacle position is found (is_obstacle returns False).\n",
        "\n",
        "  Design significance: Ensure that the enemy will not be generated on the wall.\n",
        "\n",
        "*  move_towards Move to the target:\n",
        "\n",
        "  Input: target coordinates (target_x, target_y) (usually the position of Pac-Man).\n",
        "\n",
        "  Calculate the horizontal distance dx and the vertical distance dy.\n",
        "\n",
        "  Prioritize horizontal movement: If |dx| > |dy|, move horizontally one step (left/right).\n",
        "\n",
        "  Otherwise move vertically: Move vertically one step (up/down).\n",
        "\n",
        "  Before moving, check whether the target position is an obstacle. If not, update the coordinates.\n",
        "\n",
        "*  smart_move Intelligent Movement:\n",
        "\n",
        "  70% probability to track the player: call move_towards(pacman.x, pacman.y).\n",
        "\n",
        "  30% probability to move randomly: call move_random().\n",
        "\n",
        "  Design significance: Mixed strategies prevent the enemy's behavior from being completely predictable and increase the difficulty of the game.\n",
        "\n",
        "*  move_random Random Movement:\n",
        "\n",
        "  Randomly select a direction (UP/DOWN/LEFT/RIGHT).\n",
        "\n",
        "  Check whether the target location is accessible, and update the coordinates if it is feasible.\n",
        "\n",
        "  Purpose: Simulate the enemy's \"patrol\" behavior to prevent the player from easily bypassing it"
      ],
      "metadata": {
        "id": "euCK0fGjHHsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enemy class\n",
        "class Enemy:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        while True:\n",
        "            self.x = random.randint(1, GRID_WIDTH - 2)\n",
        "            self.y = random.randint(1, GRID_HEIGHT - 2)\n",
        "            if not is_obstacle(self.x, self.y):\n",
        "                break\n",
        "\n",
        "    def move_towards(self, target_x, target_y):\n",
        "        dx = target_x - self.x\n",
        "        dy = target_y - self.y\n",
        "        if abs(dx) > abs(dy):\n",
        "            new_x = self.x + (1 if dx > 0 else -1)\n",
        "            if not is_obstacle(new_x, self.y):\n",
        "                self.x = new_x\n",
        "        else:\n",
        "            new_y = self.y + (1 if dy > 0 else -1)\n",
        "            if not is_obstacle(self.x, new_y):\n",
        "                self.y = new_y\n",
        "\n",
        "    def smart_move(self, pacman):\n",
        "        if random.random() < 0.7:\n",
        "            self.move_towards(pacman.x, pacman.y)\n",
        "        else:\n",
        "            self.move_random()\n",
        "\n",
        "    def move_random(self):\n",
        "        action = random.choice(actions)\n",
        "        new_x, new_y = self.x, self.y\n",
        "        if action == 'UP' and not is_obstacle(self.x, self.y - 1):\n",
        "            new_y -= 1\n",
        "        elif action == 'DOWN' and not is_obstacle(self.x, self.y + 1):\n",
        "            new_y += 1\n",
        "        elif action == 'LEFT' and not is_obstacle(self.x - 1, self.y):\n",
        "            new_x -= 1\n",
        "        elif action == 'RIGHT' and not is_obstacle(self.x + 1, self.y):\n",
        "            new_x += 1\n",
        "        self.x, self.y = new_x, new_y\n"
      ],
      "metadata": {
        "id": "dG3zNmQsBJO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***12.Reward function***\n",
        "*  Reward Shaping\n",
        "  Guide agent behavior through multi-dimensional rewards: food collection > safe distance > exploration reward\n",
        "  Exponential decay design makes close targets have a greater impact\n",
        "\n",
        "  \n",
        "*  State space compression\n",
        "  Discretize continuous coordinates into modulo 3 values ​​to reduce state space complexity\n",
        "  Handle food/enemy quantity in stages to prevent dimensionality explosion\n",
        "\n",
        "\n",
        "*  Dynamic difficulty adjustment\n",
        "  The less remaining food, the higher the reward, which encourages rapid collection in the later stage\n",
        "  Enemy distance penalty increases nonlinearly with proximity\n",
        "\n",
        "\n",
        "*  Exploration incentive mechanism\n",
        "  New area reward is negatively correlated with global exploration ratio to avoid over-exploration in the later stage\n",
        "  Path diversity reward prevents local loops"
      ],
      "metadata": {
        "id": "-Q7AlMVpH3Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward function\n",
        "def calculate_reward(pacman, food, enemies):\n",
        "    reward = 2  # Basic Movement Rewards\n",
        "    px, py = pacman.x, pacman.y\n",
        "    remaining = len(food.positions)\n",
        "\n",
        "    if food.collect(px, py):\n",
        "        base = 300 + 50 * (10 - remaining)\n",
        "        if remaining < 5: base *= 3\n",
        "        reward += base\n",
        "\n",
        "    enemy_dist = min((abs(e.x - px) + abs(e.y - py) for e in enemies)) if enemies else 15\n",
        "    gold_dist = min((abs(p[0] - px) + abs(p[1] - py) for p in food.positions)) if food.positions else 15\n",
        "\n",
        "    reward += 50 * np.exp(-gold_dist / 3)\n",
        "    reward -= 40 * np.exp(-enemy_dist / 1.5)\n",
        "\n",
        "    visit_ratio = len(pacman.visited) / 200\n",
        "    if (px, py) not in pacman.visited:\n",
        "        reward += 35 * (1 - visit_ratio) ** 2\n",
        "    else:\n",
        "        reward -= max(1, 3 * visit_ratio)\n",
        "\n",
        "    if len(pacman.path_history) >= 3:\n",
        "        if pacman.path_history[-1] != pacman.path_history[-3]:\n",
        "            reward += 10\n",
        "\n",
        "    if enemy_dist < 6:\n",
        "        reward -= 60 * (6 - enemy_dist) ** 1.5\n",
        "\n",
        "    if not food.positions and (px, py) == (GRID_WIDTH // 2, GRID_HEIGHT // 2):\n",
        "        reward += 800\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "urSVAXGtBJ3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***13.Game object initialization***\n",
        "\n",
        "\n",
        "  *  Pacman includes location, movement logic and status records (such as access history).\n",
        "  *  Food manages food location and provides reset and collection methods.\n",
        "  *  Enemy implements a mixed strategy with a 70% probability of tracking Pacman and a 30% random movement.\n",
        "  *  InfoPanel displays training indicators such as round number, score, exploration rate, etc. in real time.\n",
        "\n",
        "Q-learning parameter settings"
      ],
      "metadata": {
        "id": "yeOPfg0hIWfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Game object initialization\n",
        "pacman = Pacman()\n",
        "food = Food()\n",
        "enemies = [Enemy() for _ in range(2)]\n",
        "info_panel = InfoPanel()\n",
        "\n",
        "# Status Dimension\n",
        "state_dims = (3, 3, 2, 2, 5, 2, 3, 5)\n",
        "q_table = DoubleQTable(state_dims, len(actions))\n",
        "\n",
        "# Training parameters\n",
        "max_episodes = 1000\n",
        "max_steps = 1000\n",
        "episode_rewards = []\n",
        "episode_avg_q = []\n",
        "success_count = []\n",
        "epsilon_history = []\n",
        "log_messages = deque(maxlen=5)"
      ],
      "metadata": {
        "id": "ltI3buXnBMU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***14.Training loop framework***\n",
        "\n",
        "\n",
        "Function: Control the overall training rounds (max_episodes), reset the game environment (Pac-Man, food, enemy position) in each round.\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "   1.  total_reward: Accumulate the reward of this round.\n",
        "\n",
        "   2.  episode_q: Store the Q value update error of this round (for subsequent analysis).\n",
        "\n",
        "   3.  success: Mark whether the goal is successfully completed in this round.\n",
        "\n",
        "*   Action selection (ε-greedy strategy)\n",
        "\n",
        "Function: According to the current exploration rate epsilon, randomly select actions (exploration) with a certain probability, otherwise select the action with the largest Q value (exploitation).\n",
        "\n",
        "Key parameters:\n",
        "epsilon: Dynamically adjusted exploration rate, initially 1.0, decays with training.\n",
        "\n",
        "*  Environment interaction and reward calculation\n",
        "\n",
        "\n",
        "Function:\n",
        "Execute actions and update Pac-Man position.\n",
        "\n",
        "The enemy moves according to the mixed strategy (70% tracking + 30% random).\n",
        "\n",
        "Calculate instant rewards, including gold coin collection, distance penalty, exploration reward, etc.\n",
        "\n",
        "Determine the termination condition: Fail to touch the enemy, or collect all gold coins and return to the starting point successfully.\n",
        "\n",
        "*   Experience Replay (Prioritized Experience Replay, PER)\n",
        "Function:\n",
        "\n",
        "Store experience: Store each step (state, action, reward, new state, termination flag, TD error) in the buffer.\n",
        "\n",
        "*   Priority sampling: Sample experience according to the priority of TD error to accelerate the learning of important experience.\n",
        "\n",
        "Q value update: Use dual Q networks (main and target) to calculate the target value and update the main network parameters.\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "  1.  gamma = 0.99: discount factor, balancing the importance of future rewards.\n",
        "\n",
        "  2.  alpha: dynamically adjusted learning rate.\n",
        "\n",
        "  3.  batch_size = 256: the number of experiences sampled each time."
      ],
      "metadata": {
        "id": "G9sW5OYhJKlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing the training loop\n",
        "for episode in range(1, max_episodes + 1):\n",
        "    pacman.reset()\n",
        "    food.reset()\n",
        "    for e in enemies: e.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    episode_q = []\n",
        "    success = 0\n",
        "\n",
        "    while steps < max_steps:\n",
        "        state = pacman.get_state()\n",
        "\n",
        "        # ε-贪婪策略\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(actions)\n",
        "            action_idx = actions.index(action)\n",
        "        else:\n",
        "            action_idx = np.argmax(q_table.main[state])\n",
        "            action = actions[action_idx]\n",
        "\n",
        "        # Execute an action\n",
        "        pacman.move(action)\n",
        "\n",
        "        # Enemy moves\n",
        "        for e in enemies: e.smart_move(pacman)\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = calculate_reward(pacman, food, enemies)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Get new state\n",
        "        next_state = pacman.get_state()\n",
        "        done = any(pacman.x == e.x and pacman.y == e.y for e in enemies)\n",
        "        if not food.positions and not done:\n",
        "            success = 1\n",
        "            done = True\n",
        "\n",
        "        # Store experience\n",
        "        td_error = abs(reward + gamma * np.max(q_table.target[next_state]) - q_table.main[state][action_idx])\n",
        "        replay_buffer.add((state, action_idx, reward, next_state, done, td_error))\n",
        "\n",
        "        # Replay experience\n",
        "        if len(replay_buffer.buffer) >= batch_size:\n",
        "            batch, indices, weights = replay_buffer.sample(batch_size)\n",
        "            new_priorities = []\n",
        "\n",
        "            # Use enumerate to get index i and sample exp\n",
        "            for i, exp in enumerate(batch):\n",
        "                s, a_idx, r, ns, d, _ = exp\n",
        "\n",
        "                # State clipping (make sure the dimensions are correct)\n",
        "                s = tuple(\n",
        "                    max(0, min(int(x), dim_size - 1))\n",
        "                    for x, dim_size in zip(s, state_dims))\n",
        "\n",
        "                ns = tuple(\n",
        "                    max(0, min(int(x), dim_size - 1))\n",
        "                    for x, dim_size in zip(ns, state_dims))\n",
        "\n",
        "\n",
        "                # Calculate target value and TD error\n",
        "                target = r + (1 - d) * gamma * np.max(q_table.target[ns])\n",
        "                delta = abs(target - q_table.main[s][a_idx])\n",
        "\n",
        "                # Use weights[i] instead of indices.index(exp)\n",
        "                q_table.main[s][a_idx] += alpha * (target - q_table.main[s][a_idx]) * weights[i]\n",
        "\n",
        "                new_priorities.append(delta + 1e-5)\n",
        "                episode_q.append(delta)\n",
        "\n",
        "                # Update priority\n",
        "                for idx, prio in zip(indices, new_priorities):\n",
        "                    replay_buffer.priorities[idx] = prio\n",
        "\n",
        "        # Synchronize target network\n",
        "        if steps % target_update == 0:\n",
        "            q_table.update_target()\n",
        "\n",
        "        # Rendering\n",
        "        screen.fill(COLORS['GRAY'])\n",
        "        for y, row in enumerate(maze):\n",
        "            for x, char in enumerate(row):\n",
        "                if char == '#':\n",
        "                    pygame.draw.rect(screen, COLORS['BROWN'], (x * GRID_SIZE, y * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "        for pos in food.positions:\n",
        "            screen.blit(gold_img, (pos[0] * GRID_SIZE, pos[1] * GRID_SIZE))\n",
        "        screen.blit(player_img, (pacman.x * GRID_SIZE, pacman.y * GRID_SIZE))\n",
        "        for e in enemies:\n",
        "            screen.blit(enemy_img, (e.x * GRID_SIZE, e.y * GRID_SIZE))\n",
        "\n",
        "        avg_q = np.mean(episode_q) if episode_q else 0\n",
        "        info_panel.draw(screen, episode, total_reward, epsilon, steps, avg_q, success_count)\n",
        "        pygame.display.flip()\n",
        "        clock.tick(FPS)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        steps += 1"
      ],
      "metadata": {
        "id": "v_rO3sUWBO9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***15.Dynamic parameter adjustment***\n",
        "  1.  Function:\n",
        "\n",
        "  Learning rate adjustment: gradually reduce the learning rate through cosine annealing to balance training stability and speed.\n",
        "\n",
        "  2.  Exploration rate adjustment:\n",
        "\n",
        "  Basic decay: epsilon *= 0.9995, gradually reduce the exploration rate.\n",
        "\n",
        "  Dynamic adjustment: If the success rate of the last 100 rounds is less than 25% (success_threshold = 0.25), temporarily increase the exploration rate (epsilon *= 1.3) to prevent local optimality."
      ],
      "metadata": {
        "id": "qHpTTSxFJsKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Dynamic parameter adjustment\n",
        "    alpha = max(min_alpha, 0.5 * (1 + np.cos(episode / 500 * np.pi)) * 0.7)\n",
        "    if len(success_count) >= 100 and np.mean(success_count[-100:]) > 0.3:\n",
        "        alpha *= 0.995\n",
        "\n",
        "    # εStrategy Adjustment\n",
        "    epsilon = max(adaptive_epsilon_min, epsilon * epsilon_decay)\n",
        "    if episode % 100 == 0 and len(success_count) >= 100:\n",
        "        recent_success = np.mean(success_count[-100:])\n",
        "        if recent_success < success_threshold:\n",
        "            epsilon = min(0.3, epsilon * boost_factor)\n",
        "            adaptive_epsilon_min = min(0.05, adaptive_epsilon_min * 1.1)\n",
        "        else:\n",
        "            adaptive_epsilon_min = max(0.005, adaptive_epsilon_min * 0.95)\n",
        "\n",
        "    if episode % 400 == 0:\n",
        "        epsilon = min(0.4, max(adaptive_epsilon_min, epsilon * 1.3))\n"
      ],
      "metadata": {
        "id": "RvPRqITiBSh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***16.Data recording and model saving***\n",
        "\n",
        "\n",
        "  Function:\n",
        "\n",
        "  Record the success status, cumulative rewards, average Q value changes and exploration rate of each round.\n",
        "\n",
        "  Save Q table parameters (q_table_ep{episode}.npy) every 100 rounds for subsequent analysis and continued training."
      ],
      "metadata": {
        "id": "HXYZO3ihJ49d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recording Data\n",
        "    success_count.append(success)\n",
        "    episode_rewards.append(total_reward)\n",
        "    episode_avg_q.append(np.mean(episode_q) if episode_q else 0)\n",
        "    epsilon_history.append(epsilon)\n",
        "\n",
        "    # Logging\n",
        "    log_msg = f\"Ep {episode} | Reward: {total_reward:.1f} | Avg QΔ: {episode_avg_q[-1]:.2f} | ε: {epsilon:.4f} | Success: {success}\"\n",
        "    logging.info(log_msg)\n",
        "    log_messages.append(log_msg)\n",
        "\n",
        "    # Save the model regularly\n",
        "    if episode % 100 == 0:\n",
        "        np.save(f\"q_table_ep{episode}.npy\", q_table.main)"
      ],
      "metadata": {
        "id": "Y0cxXFOABVR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***17.Visualization and Rendering***\n",
        "\n",
        "\n",
        "  1.  Features:\n",
        "\n",
        "  Use Pygame to render mazes, coins, Pac-Man, and enemies in real time.\n",
        "\n",
        "  Information panel displays training metrics (rounds, rewards, exploration rate, etc.).\n",
        "\n",
        "  Frame rate is locked to 30 FPS to ensure smooth interaction.\n",
        "  2.  Draw four charts during the training process:\n",
        "\n",
        "\n",
        "  Reward trend chart: shows the original reward and moving average.\n",
        "\n",
        "\n",
        "  Q value dynamic chart: shows the change of Q value.\n",
        "\n",
        "\n",
        "  Exploration rate evolution chart: shows the change of epsilon.\n",
        "\n",
        "  \n",
        "  Maze exploration heat map: shows the distribution of Pac-Man's visit times."
      ],
      "metadata": {
        "id": "QLjBVMlTKDk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization enhancement\n",
        "def plot_training():\n",
        "    plt.figure(figsize=(18, 12))\n",
        "    plt.style.use('seaborn-darkgrid')\n",
        "\n",
        "    # 1. Reward Trend Chart\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(episode_rewards, alpha=0.3, color='#1f77b4', label='Raw')\n",
        "    reward_ma = np.convolve(episode_rewards, np.ones(100)/100, mode='valid')\n",
        "    plt.plot(reward_ma, color='#ff7f0e', linewidth=2, label='MA100')\n",
        "    plt.title(\"Reward Trend\", fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.4)\n",
        "\n",
        "    # 2. Q value dynamic diagram\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(episode_avg_q, color='#2ca02c', linewidth=2)\n",
        "    plt.title(\"Q-Value Dynamics\", fontsize=14)\n",
        "    plt.grid(alpha=0.4)\n",
        "\n",
        "    # 3. Exploration rate evolution graph\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epsilon_history, color='#d62728')\n",
        "    plt.yscale('log')\n",
        "    plt.title(\"Exploration Rate (ε)\", fontsize=14)\n",
        "    plt.grid(alpha=0.4)\n",
        "\n",
        "    # 4. Maze exploration heat map\n",
        "    plt.subplot(2, 2, 4)\n",
        "    log_visited = np.log1p(pacman.visited_grids.T)\n",
        "    sns.heatmap(\n",
        "        log_visited,\n",
        "        cmap=\"YlGnBu\",\n",
        "        annot=False,\n",
        "        cbar_kws={'label': 'log(Visits + 1)'},\n",
        "        square=True,\n",
        "        linewidths=0.5,\n",
        "        linecolor='white'\n",
        "    )\n",
        "    plt.title(f\"Maze Exploration Heatmap (Episodes={max_episodes})\", fontsize=14)\n",
        "    plt.xticks(\n",
        "        np.arange(0, GRID_WIDTH, 5),\n",
        "        labels=np.arange(0, GRID_WIDTH, 5) * GRID_SIZE,\n",
        "        rotation=45\n",
        "    )\n",
        "    plt.yticks(\n",
        "        np.arange(0, GRID_HEIGHT, 5),\n",
        "        labels=np.arange(0, GRID_HEIGHT, 5) * GRID_SIZE\n",
        "    )\n",
        "    plt.xlabel(\"X Coordinate (pixels)\", fontsize=12)\n",
        "    plt.ylabel(\"Y Coordinate (pixels)\", fontsize=12)\n",
        "\n",
        "    # Unified storage\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_report_v2.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_training()\n",
        "\n",
        "# Keep Window\n",
        "running = True\n",
        "while running:\n",
        "    for event in pygame.event.get():\n",
        "        if event.type == pygame.QUIT:\n",
        "            running = False\n",
        "    clock.tick(5)\n",
        "\n",
        "pygame.quit()"
      ],
      "metadata": {
        "id": "CELFvPRGBXkJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}